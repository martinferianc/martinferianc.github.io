[{"authors":null,"categories":null,"content":"Martin obtained an MEng in Electronic and Information Engineering from Imperial College London, London, UK in 2015. He is currently a PhD candidate in the Department of Electronic and Electrical Engineering at University College London. His research interests include Bayesian neural networks, deep learning , hardware acceleration and confidence calibration. He has hands-on experience from industrial/academic placements in different countries.\n  Download my resumé/CV.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Martin obtained an MEng in Electronic and Information Engineering from Imperial College London, London, UK in 2015. He is currently a PhD candidate in the Department of Electronic and Electrical Engineering at University College London.","tags":null,"title":"Martin Ferianc","type":"authors"},{"authors":["Vittorio Casagrande","Martin Ferianc","Miguel Rodrigues","Francesca Boem"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690668798,"objectID":"efbe70148e9e73ce1eda9ae65a3e21f6","permalink":"https://martinferianc.github.io/publication/casagrande-2023-online/","publishdate":"2023-07-29T22:13:18.136251Z","relpermalink":"/publication/casagrande-2023-online/","section":"publication","summary":"We propose a novel Model Predictive Control (MPC) scheme based on online-learning (OL) for microgrid energy management, where the control optimisation is embedded as the last layer of the neural network. The proposed MPC scheme deals with uncertainty on the load and renewable generation power profiles and on electricity prices, by employing the predictions provided by an online trained neural network in the optimisation problem. In order to adapt to possible changes in the environment, the neural network is online trained based on continuously received data. The network hyperparameters are selected by performing a hyperparameter optimisation before the deployment of the controller, using a pretraining dataset. We show the effectiveness of the proposed method for microgrid energy management through extensive experiments on real microgrid datasets. Moreover, we show that the proposed algorithm has good transfer learning (TL) capabilities among different microgrids.","tags":[],"title":"An Online Learning Method for Microgrid Energy Management Control","type":"publication"},{"authors":["Martin Ferianc","Ondrej Bohdal","Timothy Hospedales","Miguel Rodrigues"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690668797,"objectID":"927ce80492f7db8feb8692ca962c566f","permalink":"https://martinferianc.github.io/publication/ferianc-2023-impact/","publishdate":"2023-07-29T22:13:17Z","relpermalink":"/publication/ferianc-2023-impact/","section":"publication","summary":"Noise injection and data augmentation strategies have been effective for enhancing the generalisation and robustness of neural networks (NNs). Certain types of noise such as label smoothing and MixUp have also been shown to improve calibration. Since noise can be added in various stages of the NN's training, it motivates the question of when and where the noise is the most effective. We study a variety of noise types to determine how much they improve calibration and generalisation, and under what conditions. More specifically we evaluate various noise-injection strategies in both in-distribution (ID) and out-of-distribution (OOD) scenarios. The findings highlight that activation noise was the most transferable and effective in improving generalisation, while input augmentation noise was prominent in improving calibration on OOD but not necessarily ID data.","tags":[],"title":"Impact of Noise on Calibration and Generalisation of Neural Networks","type":"publication"},{"authors":["Martin Ferianc","Miguel Rodrigues"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690668798,"objectID":"c586a5d5ef7f421bd04016df0705dd72","permalink":"https://martinferianc.github.io/publication/ferianc-2023-mimmo/","publishdate":"2023-07-29T22:13:18Z","relpermalink":"/publication/ferianc-2023-mimmo/","section":"publication","summary":"Neural networks (NNs) have achieved superhuman accuracy in multiple tasks, but NNs predictions' certainty is often debatable, especially if confronted with out of training distribution data. Averaging predictions of an ensemble of NNs can recalibrate the certainty of the predictions, but an ensemble is computationally expensive to deploy in practice. Recently, a new hardware-efficient multi-input multi-output (MIMO) NN was proposed to fit an ensemble of independent NNs into a single NN. In this work, we propose the addition of early-exits to the MIMO architecture with inferred depth-wise weightings to produce multiple predictions for the same input, giving a more diverse ensemble. We denote this combination as MIMMO: a multi-input, massive multi-output NN and we show that it can achieve better accuracy and calibration compared to the MIMO NN, simultaneously fit more NNs and be similarly hardware efficient as MIMO or the early-exit ensemble.","tags":[],"title":"MIMMO: Multi-Input Massive Multi-Output Neural Network","type":"publication"},{"authors":["Martin Wistuba","Martin Ferianc","Lukas Balles","Cédric Archambeau","Giovanni Zappella"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690668798,"objectID":"1de563885e7eb5df983472c356d1edf6","permalink":"https://martinferianc.github.io/publication/wistuba-2023-renate/","publishdate":"2023-07-29T22:13:18.211773Z","relpermalink":"/publication/wistuba-2023-renate/","section":"publication","summary":"Continual learning enables the incremental training of machine learning models on non-stationary data streams.While academic interest in the topic is high, there is little indication of the use of state-of-the-art continual learning algorithms in practical machine learning deployment. This paper presents Renate, a continual learning library designed to build real-world updating pipelines for PyTorch models. We discuss requirements for the use of continual learning algorithms in practice, from which we derive design principles for Renate. We give a high-level description of the library components and interfaces. Finally, we showcase the strengths of the library by presenting experimental results. Renate may be found at https://github.com/awslabs/renate.","tags":[],"title":"Renate: A library for real-world continual learning","type":"publication"},{"authors":["Martin Ferianc","Miguel Rodrigues"],"categories":null,"content":"","date":1652918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652918400,"objectID":"ed56dbb7fb7536448b55dda790673428","permalink":"https://martinferianc.github.io/publication/ferianc-2022-simple/","publishdate":"2022-05-19T18:28:51.635151Z","relpermalink":"/publication/ferianc-2022-simple/","section":"publication","summary":"Considering uncertainty estimation of modern neural networks (NNs) is one of the most important steps towards deploying machine learning systems to meaningful real-world applications such as in medicine, finance or autonomous systems. At the moment, ensembles of different NNs constitute the state-of-the-art in both accuracy and uncertainty estimation in different tasks. However, ensembles of NNs are unpractical under real-world constraints, since their computation and memory consumption scale linearly with the size of the ensemble, which increase their latency and deployment cost. In this work, we examine a simple regularisation approach for distribution-free knowledge distillation of ensemble of machine learning models into a single NN. The aim of the regularisation is to preserve the diversity, accuracy and uncertainty estimation characteristics of the original ensemble without any intricacies, such as fine-tuning. We demonstrate the generality of the approach on combinations of toy data, SVHN/CIFAR-10, simple to complex NN architectures and different tasks.","tags":null,"title":"Simple Regularisation for Uncertainty-Aware Knowledge Distillation","type":"publication"},{"authors":["Hongxiang Fan","Martin Ferianc","Zhiqiang Que","Xinyu Niu","Miguel Rodrigues","Wayne Luk"],"categories":null,"content":"","date":1649009963,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649009963,"objectID":"e2c1d28edf786bb814af8f2778dbf60c","permalink":"https://martinferianc.github.io/publication/9720069/","publishdate":"2022-04-03T18:19:23.551513Z","relpermalink":"/publication/9720069/","section":"publication","summary":"Bayesian neural networks (BayesNNs) have demonstrated their advantages in various safety-critical applications, such as autonomous driving or healthcare, due to their ability to capture and represent model uncertainty. However, standard BayesNNs require to be repeatedly run because of Monte Carlo sampling to quantify their uncertainty, which puts a burden on their real-world hardware performance. To address this performance issue, this paper systematically exploits the extensive structured sparsity and redundant computation in BayesNNs. Different from the unstructured or structured sparsity existing in standard convolutional NNs, the structured sparsity of BayesNNs is introduced by Monte Carlo Dropout and its associated sampling required during uncertainty estimation and prediction, which can be exploited through both algorithmic and hardware optimizations. We first classify the observed sparsity patterns into three categories: dropout sparsity, layer sparsity and sample sparsity. On the algorithmic side, a framework is proposed to automatically explore these three sparsity categories without sacrificing algorithmic performance. We demonstrated that structured sparsity can be exploited to accelerate CPU designs by up to 49 times, and GPU designs by up to 40 times. On the hardware side, a novel hardware architecture is proposed to accelerate BayesNNs, which achieves a high hardware performance using the runtime adaptable hardware engines and the intelligent skipping support. Upon implementing the proposed hardware design on an FPGA, our experiments demonstrated that the algorithm-optimized BayesNNs can achieve up to 56 times speedup when compared with unoptimized Bayesian nets. Comparing with the optimized GPU implementation, our FPGA design achieved up to 7.6 times speedup and up to 39.3 times higher energy efficiency","tags":[""],"title":"Accelerating Bayesian Neural Networks via Algorithmic and Hardware Optimizations","type":"publication"},{"authors":["Hongxiang Fan","Martin Ferianc","Zhiqiang Que","Shuanglong Liu","Xinyu Niu","Miguel Rodrigues","Wayne Luk"],"categories":null,"content":"","date":1649009963,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649009963,"objectID":"3dc1548867434e247ae04ed960fae1d0","permalink":"https://martinferianc.github.io/publication/9743481/","publishdate":"2022-04-03T18:19:23.551731Z","relpermalink":"/publication/9743481/","section":"publication","summary":"Neural networks (NNs) have demonstrated their potential in a variety of domains ranging from computer vision to natural language processing. Among various NNs, two-dimensional (2D) and three-dimensional (3D) convolutional neural networks (CNNs) have been widely adopted for a broad spectrum of applications such as image classification and video recognition, due to their excellent capabilities in extracting 2D and 3D features. However, standard 2D and 3D CNNs are not able to capture their model uncertainty which is crucial for many safety-critical applications including healthcare and autonomous driving. In contrast, Bayesian convolutional neural networks (BayesCNNs), as a variant of CNNs, have demonstrated their ability to express uncertainty in their prediction via a mathematical grounding. Nevertheless, BayesCNNs have not been widely used in industrial practice due to their compute requirements stemming from sampling and subsequent forward passes through the whole network multiple times. As a result, these requirements significantly increase the amount of computation and memory consumption in comparison to standard CNNs. This paper proposes a novel FPGA-based hardware architecture to accelerate both 2D and 3D BayesCNNs based on Monte Carlo Dropout. Compared with other state-of-the-art accelerators for BayesCNNs, the proposed design can achieve up to 4 times higher energy efficiency and 9 times better compute efficiency. An automatic framework capable of supporting partial Bayesian inference is proposed to explore the trade-off between algorithm and hardware performance. Extensive experiments are conducted to demonstrate that our framework can effectively find the optimal implementations in the design space.","tags":[""],"title":"FPGA-based Acceleration for Bayesian Convolutional Neural Networks","type":"publication"},{"authors":["Hongxiang Fan","Martin Ferianc","Zhiqiang Que","He Li","Shuanglong Liu","Xinyu Niu","Wayne Luk"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"75a5b0ddd461f43b846a059bd98f56f0","permalink":"https://martinferianc.github.io/publication/9712541/","publishdate":"2022-04-03T18:19:23.545759Z","relpermalink":"/publication/9712541/","section":"publication","summary":"Recent advances in algorithm-hardware co-design for deep neural networks (DNNs) have demonstrated their potential in automatically designing neural architectures and hardware designs. Nevertheless, it is still a challenging optimization problem due to the expensive training cost and the time-consuming hardware implementation, which makes the exploration on the vast design space of neural architecture and hardware design intractable. In this paper, we demonstrate that our proposed approach is capable of locating designs on the Pareto frontier. This capability is enabled by a novel three-phase co-design framework, with the following new features: (a) decoupling DNN training from the design space exploration of hardware architecture and neural architecture, (b) providing a hardware-friendly neural architecture space by considering hardware characteristics in constructing the search cells, (c) adopting Gaussian process to predict accuracy, latency and power consumption to avoid time-consuming synthesis and place-and-route processes. In comparison with the manually-designed ResNet101, InceptionV2 and MobileNetV2, we can achieve up to 5% higher accuracy with up to $3times$ speed up on the ImageNet dataset. Compared with other state-of-the-art co-design frameworks, our found network and hardware configuration can achieve 2% (~ 6% higher accuracy, $2times∼ 26times$ smaller latency and $8.5times$ higher energy efficiency.","tags":[""],"title":"Algorithm and Hardware Co-design for Reconfigurable CNN Accelerator","type":"publication"},{"authors":["Hongxiang Fan","Martin Ferianc","Wayne Luk"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663526565,"objectID":"6b76cbfcb17c3d270c78a3b6f2c42781","permalink":"https://martinferianc.github.io/publication/fan-2022-enabling/","publishdate":"2022-09-18T18:42:45.384958Z","relpermalink":"/publication/fan-2022-enabling/","section":"publication","summary":"Quantifying the uncertainty of neural networks (NNs) has been required by many safety-critical applications such as autonomous driving or medical diagnosis. Recently, Bayesian transformers have demonstrated their capabilities in providing high-quality uncertainty estimates paired with excellent accuracy. However, their real-time deployment is limited by the compute-intensive attention mechanism that is core to the transformer architecture, and the repeated Monte Carlo sampling to quantify the predictive uncertainty. To address these limitations, this paper accelerates Bayesian transformers via both algorithmic and hardware optimizations. On the algorithmic level, an evolutionary algorithm (EA)-based framework is proposed to exploit the sparsity in Bayesian transformers and ease their computational workload. On the hardware level, we demonstrate that the sparsity brings hardware performance improvement on our optimized CPU and GPU implementations. An adaptable hardware architecture is also proposed to accelerate Bayesian transformers on an FPGA. Extensive experiments demonstrate that the EA-based framework, together with hardware optimizations, reduce the latency of Bayesian transformers by up to 13, 12 and 20 times on CPU, GPU and FPGA platforms respectively, while achieving higher algorithmic performance.","tags":[],"title":"Enabling fast uncertainty estimation: accelerating bayesian transformers via algorithmic and hardware optimizations","type":"publication"},{"authors":["Hongxiang Fan","Martin Ferianc","Miguel Rodrigues","Hongyu Zhou","Xinyu Niu","Wayne Luk"],"categories":null,"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"9365af2b98c94b51263cf55ff694f234","permalink":"https://martinferianc.github.io/publication/9586137/","publishdate":"2022-04-03T18:19:23.550943Z","relpermalink":"/publication/9586137/","section":"publication","summary":"Neural networks (NNs) have demonstrated their potential in a wide range of applications such as image recognition, decision making or recommendation systems. However, standard NNs are unable to capture their model uncertainty which is crucial for many safety-critical applications including healthcare and autonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to express uncertainty in their prediction via a mathematical grounding. Nevertheless, BNNs have not been as widely used in industrial practice, mainly because of their expensive computational cost and limited hardware performance. This work proposes a novel FPGA based hardware architecture to accelerate BNNs inferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN accelerators, the proposed accelerator can achieve up to 4 times higher energy efficiency and 9 times better compute efficiency. Considering partial Bayesian inference, an automatic framework is proposed, which explores the trade-off between hardware and algorithmic performance. Extensive experiments are conducted to demonstrate that our proposed framework can effectively find the optimal points in the design space.","tags":[""],"title":"High-Performance FPGA-based Accelerator for Bayesian Neural Networks","type":"publication"},{"authors":["Martin Ferianc","Zhiqiang Que","Hongxiang Fan","Wayne Luk","Miguel Rodrigues"],"categories":null,"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"3a1272178c4b3e15f68485f1746058cd","permalink":"https://martinferianc.github.io/publication/9609847/","publishdate":"2022-04-03T18:31:28.794503Z","relpermalink":"/publication/9609847/","section":"publication","summary":"Neural networks have demonstrated their outstanding performance in a wide range of tasks. Specifically recurrent architectures based on long-short term memory (LSTM) cells have manifested excellent capability to model time dependencies in real-world data. However, standard recurrent architectures cannot estimate their uncertainty which is essential for safety-critical applications such as in medicine. In contrast, Bayesian recurrent neural networks (RNNs) are able to provide uncertainty estimation with improved accuracy. Nonetheless, Bayesian RNNs are computationally and memory demanding, which limits their practicality despite their advantages. To address this issue, we propose an FPGA-based hardware design to accelerate Bayesian LSTM-based RNNs. To further improve the overall algorithmic-hardware performance, a co-design framework is proposed to explore the most fitting algorithmic-hardware configurations for Bayesian RNNs. We conduct extensive experiments on healthcare applications to demonstrate the improvement of our design and the effectiveness of our framework. Compared with GPU implementation, our FPGA-based design can achieve up to 10 times speedup with nearly 106 times higher energy efficiency. To the best of our knowledge, this is the first work targeting acceleration of Bayesian RNNs on FPGAs.","tags":[""],"title":"Optimizing Bayesian Recurrent Neural Networks on an FPGA-based Accelerator","type":"publication"},{"authors":["Shuanglong Liu","Hongxiang Fan","Martin Ferianc","Xinyu Niu","Huifeng Shi","Wayne Luk"],"categories":null,"content":"","date":1612203709,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612203709,"objectID":"54b26d0197ecd9f27c8691b6f009d6de","permalink":"https://martinferianc.github.io/publication/liu-2021-toward/","publishdate":"2021-02-01T18:21:49.338226Z","relpermalink":"/publication/liu-2021-toward/","section":"publication","summary":"Due to the huge success and rapid development of convolutional neural networks (CNNs), there is a growing demand for hardware accelerators that accommodate a variety of CNNs to improve their inference latency and energy efficiency, in order to enable their deployment in real-time applications. Among popular platforms, field-programmable gate arrays (FPGAs) have been widely adopted for CNN acceleration because of their capability to provide superior energy efficiency and low-latency processing, while supporting high reconfigurability, making them favorable for accelerating rapidly evolving CNN algorithms. This article introduces a highly customized streaming hardware architecture that focuses on improving the compute efficiency for streaming applications by providing full-stack acceleration of CNNs on FPGAs. The proposed accelerator maps most computational functions, that is, convolutional and deconvolutional layers into a singular unified module, and implements the residual and concatenative connections between the functions with high efficiency, to support the inference of mainstream CNNs with different topologies. This architecture is further optimized through exploiting different levels of parallelism, layer fusion, and fully leveraging digital signal processing blocks (DSPs). The proposed accelerator has been implemented on Intel's Arria 10 GX1150 hardware and evaluated with a wide range of benchmark models. The results demonstrate a high performance of over 1.3 TOP/s of throughput, up to 97% of compute [multiply-accumulate (MAC)] efficiency, which outperforms the state-of-the-art FPGA accelerators.","tags":[""],"title":"Toward Full-Stack Acceleration of Deep Convolutional Neural Networks on FPGAs","type":"publication"},{"authors":["Martin Ferianc","Divyansh Manocha","Hongxiang Fan","Miguel Rodrigues"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"e576e76acb10e25f1dd70b24d8b0f27a","permalink":"https://martinferianc.github.io/publication/10-1007-978-3-030-86365-4-39/","publishdate":"2022-04-03T18:37:58.20793Z","relpermalink":"/publication/10-1007-978-3-030-86365-4-39/","section":"publication","summary":"Fully convolutional U-shaped neural networks have largely been the dominant approach for pixel-wise image segmentation. In this work, we tackle two defects that hinder their deployment in real-world applications: 1) Predictions lack uncertainty quantification that may be crucial to many decision-making systems; 2) Large memory storage and computational consumption demanding extensive hardware resources. To address these issues and improve their practicality we demonstrate a few-parameter compact Bayesian convolutional architecture, that achieves a marginal improvement in accuracy in comparison to related work using significantly fewer parameters and compute operations. The architecture combines parameter-efficient operations such as separable convolutions, bilinear interpolation, multi-scale feature propagation and Bayesian inference for per-pixel uncertainty quantification through Monte Carlo Dropout. The best performing configurations required fewer than 2.5 million parameters on diverse challenging datasets with few observations.","tags":null,"title":"ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation","type":"publication"},{"authors":["Martin Ferianc","Hongxiang Fan","Divyansh Manocha","Hongyu Zhou","Shuanglong Liu","Xinyu Niu","Wayne Luk"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"789e78612a58d3cc83ca87e8147ff9ae","permalink":"https://martinferianc.github.io/publication/ferianc-2021-improving/","publishdate":"2022-04-03T18:28:51.635151Z","relpermalink":"/publication/ferianc-2021-improving/","section":"publication","summary":"Contemporary advances in neural networks (NNs) have demonstrated their potential in different applications such as in image classification, object detection or natural language processing. In particular, reconfigurable accelerators have been widely used for the acceleration of NNs due to their reconfigurability and efficiency in specific application instances. To determine the configuration of the accelerator, it is necessary to conduct design space exploration to optimize the performance. However, the process of design space exploration is time consuming because of the slow performance evaluation for different configurations. Therefore, there is a demand for an accurate and fast performance prediction method to speed up design space exploration. This work introduces a novel method for fast and accurate estimation of different metrics that are of importance when performing design space exploration. The method is based on a Gaussian process regression model parametrised by the features of the accelerator and the target NN to be accelerated. We evaluate the proposed method together with other popular machine learning based methods in estimating the latency and energy consumption of our implemented accelerator on two different hardware platforms targeting convolutional neural networks. We demonstrate improvements in estimation accuracy, without the need for significant implementation effort or tuning.","tags":null,"title":"Improving Performance Estimation for Design Space Exploration for Convolutional Neural Network Accelerators","type":"publication"},{"authors":["Martin Ferianc","Anush Sankaran","Olivier Mastropietro","Ehsan Saboori","Quentin Cappart"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6276dc32a0a297d01bc0fbf5619e1f0f","permalink":"https://martinferianc.github.io/publication/ferianc-2021-causal/","publishdate":"2022-04-03T18:24:59.627344Z","relpermalink":"/publication/ferianc-2021-causal/","section":"publication","summary":"Neural networks (NNs) are making a large impact both on research and industry. Nevertheless, as NNs' accuracy increases, it is followed by an expansion in their size, required number of compute operations and energy consumption. Increase in resource consumption results in NNs' reduced adoption rate and real-world deployment impracticality. Therefore, NNs need to be compressed to make them available to a wider audience and at the same time decrease their runtime costs. In this work, we approach this challenge from a causal inference perspective, and we propose a scoring mechanism to facilitate structured pruning of NNs. The approach is based on measuring mutual information under a maximum entropy perturbation, sequentially propagated through the NN. We demonstrate the method's performance on two datasets and various NNs' sizes, and we show that our approach achieves competitive performance under challenging conditions.","tags":null,"title":"On Causal Inference for Data-free Structured Pruning","type":"publication"},{"authors":["Martin Ferianc","Partha Maji","Matthew Mattina","Miguel Rodrigues"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"bf6fe47b6ae06643303d9e44429d5a89","permalink":"https://martinferianc.github.io/publication/ferianc-2021-effects/","publishdate":"2022-04-07T08:31:39.82688Z","relpermalink":"/publication/ferianc-2021-effects/","section":"publication","summary":"Bayesian neural networks (BNNs) are making significant progress in many research areas where decision-making needs to be accompanied by uncertainty estimation. Being able to quantify uncertainty while making decisions is essential for understanding when the model is over-/under-confident, and hence BNNs are attracting interest in safety-critical applications, such as autonomous driving, healthcare, and robotics. Nevertheless, BNNs have not been as widely used in industrial practice, mainly because of their increased memory and compute costs. In this work, we investigate quantisation of BNNs by compressing 32-bit floating-point weights and activations to their integer counterparts, that has already been successful in reducing the compute demand in standard pointwise neural networks. We study three types of quantised BNNs, we evaluate them under a wide range of different settings, and we empirically demonstrate that a uniform quantisation scheme applied to BNNs does not substantially decrease their quality of uncertainty estimation.","tags":null,"title":"On the effects of quantisation on model uncertainty in Bayesian neural networks","type":"publication"},{"authors":["Martin Ferianc","Hongxiang Fan","Miguel Rodrigues"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"a98fc7f127d8016294127a03c450781e","permalink":"https://martinferianc.github.io/publication/ferianc-2021-vinnas/","publishdate":"2022-04-03T18:19:23.546628Z","relpermalink":"/publication/ferianc-2021-vinnas/","section":"publication","summary":"In recent years, neural architecture search (NAS) has received intensive scientific and industrial interest due to its capability of finding a neural architecture with high accuracy for various artificial intelligence tasks such as image classification or object detection. In particular, gradient-based NAS approaches have become one of the more popular approaches thanks to their computational efficiency during the search. However, these methods often experience a mode collapse, where the quality of the found architectures is poor due to the algorithm resorting to choosing a single operation type for the entire network, or stagnating at a local minima for various datasets or search spaces. To address these defects, we present a differentiable variational inference-based NAS method for searching sparse convolutional neural networks. Our approach finds the optimal neural architecture by dropping out candidate operations in an over-parameterised supergraph using variational dropout with automatic relevance determination prior, which makes the algorithm gradually remove unnecessary operations and connections without risking mode collapse. The evaluation is conducted through searching two types of convolutional cells that shape the neural network for classifying different image datasets. Our method finds diverse network cells, while showing state-of-the-art accuracy with up to almost 2 times fewer non-zero parameters.","tags":null,"title":"VINNAS: Variational Inference-based Neural Network Architecture Search","type":"publication"},{"authors":["Hongxiang Fan","Martin Ferianc","Shuanglong Liu","Zhiqiang Que","Xinyu Niu","Wayne Luk"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"f1c0a17e958217741fc9904c2b4746fa","permalink":"https://martinferianc.github.io/publication/fan-2020-optimizing/","publishdate":"2022-04-03T18:33:11.709711Z","relpermalink":"/publication/fan-2020-optimizing/","section":"publication","summary":"Neural architecture search (NAS) aims to find the optimal neural network automatically for different scenarios. Among various NAS methods, the differentiable NAS (DNAS) approach has demonstrated its effectiveness in terms of searching cost and final accuracy. However, most of previous efforts focus on applying DNAS to GPU or CPU platforms, and its potential is less exploited on the FPGA. In this paper, we first propose a novel FPGA-based CNN accelerator. An accurate performance model of the proposed hardware design is also introduced. To improve accuracy as well as hardware performance, we then apply DNAS and encapsulate the proposed performance model into the objective function. Based on our FPGA design and NAS method, the experiments demonstrate that the network generated by NAS achieves nearly 95% accuracy on CIFAR-10, while decreasing latency by nearly 12 times compared with existing work.","tags":[""],"title":"Optimizing FPGA-Based CNN Accelerator Using Differentiable Neural Architecture Search","type":"publication"},{"authors":["Martin Ferianc","Hongxiang Fan","Ringo S. W. Chu","Jakub Stano","Wayne Luk"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"82cfc83e188987b84c340067d5cee854","permalink":"https://martinferianc.github.io/publication/10-1007-978-3-030-44534-8-1/","publishdate":"2022-04-03T18:36:37.267633Z","relpermalink":"/publication/10-1007-978-3-030-44534-8-1/","section":"publication","summary":"Field-programmable gate array (FPGA) based accelerators are being widely used for acceleration of convolutional neural networks (CNNs) due to their potential in improving the performance and reconfigurability for specific application instances. To determine the optimal configuration of an FPGA-based accelerator, it is necessary to explore the design space and an accurate performance prediction plays an important role during the exploration. This work introduces a novel method for fast and accurate estimation of latency based on a Gaussian process parametrised by an analytic approximation and coupled with runtime data. The experiments conducted on three different CNNs on an FPGA-based accelerator on Intel Arria 10 GX 1150 demonstrated a 30.7% improvement in accuracy with respect to the mean absolute error in comparison to a standard analytic method in leave-one-out cross-validation.","tags":null,"title":"Improving Performance Estimation for FPGA-Based Accelerators for Convolutional Neural Networks","type":"publication"},{"authors":["Hongxiang Fan","Gang Wang","Martin Ferianc","Xinyu Niu","Wayne Luk"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"15c9ee10bf3fb0b2068f38a402c5703b","permalink":"https://martinferianc.github.io/publication/fan-2019-static/","publishdate":"2022-04-03T18:28:51.634609Z","relpermalink":"/publication/fan-2019-static/","section":"publication","summary":"Convolutional neural networks (CNNs) have been widely applied in various computer vision and speech processing applications. However, the algorithmic complexity of CNNs hinders their deployment in embedded systems with limited memory and computational resources. This paper proposes static block floating-point (BFP) quantization, an effective approach involving Kullback-Leibler divergence, to determine the static shared exponents. Without need for retraining, the proposed approach is able to quantize CNNs to 8 bits with negligible accuracy loss. An FPGA-based hardware design with static BFP quantization is also proposed. Compared with 8-bit integer linear quantization, our experiments show that the hardware kernel based on static BFP quantization can achieve over 50% reduction in logic resources on an FPGA. Based on static BFP quantization, a tool implemented in the PyTorch framework is developed, which can automatically generate optimised configuration according to user requirements for given CNN models, where the entire optimization process takes only a few minutes on an Intel Xeon Silver 4110 CPU.","tags":[""],"title":"Static Block Floating-Point Quantization for Convolutional Neural Networks on FPGA","type":"publication"},{"authors":["Hongxiang Fan","Cheng Luo","Chenglong Zeng","Martin Ferianc","Zhiqiang Que","Shuanglong Liu","Xinyu Niu","Wayne Luk"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"b5d288d342b1575197c1c71c18b788f2","permalink":"https://martinferianc.github.io/publication/fan-2019-f/","publishdate":"2022-04-03T18:26:48.040818Z","relpermalink":"/publication/fan-2019-f/","section":"publication","summary":"Three-dimensional convolutional neural networks (3D CNNs) have demonstrated their outstanding classification accuracy for human action recognition (HAR). However, the large number of computations and parameters in 3D CNNs limits their deployability in real-life applications. To address this challenge, this paper adopts an algorithm-hardware co-design method by proposing an efficient 3D CNN building unit called 3D-1 bottleneck residual block (3D-1 BRB) at the algorithm level, and a corresponding FPGA-based hardware architecture called F-E3D at the hardware level. Based on 3D-1 BRB, a novel 3D CNN model called E3DNet is developed, which achieves nearly 37 times reduction in model size and 5% improvement in accuracy compared to standard 3D CNNs on the UCF101 dataset. Together with several hardware optimizations, including 3D fused BRB, online blocking and kernel reuse, the proposed F-E3D is nearly 13 times faster than a previous FPGA design for 3D CNNs, with performance and accuracy comparable to other state-of-the-art 3D CNN models on GPU platforms while requiring only 7% of their energy consumption.","tags":[""],"title":"F-E3D: FPGA-based Acceleration of an Efficient 3D Convolutional Neural Network for Human Action Recognition","type":"publication"},{"authors":["Hongxiang Fan","Shuanglong Liu","Martin Ferianc","Ho-Cheung Ng","Zhiqiang Que","Shen Liu","Xinyu Niu","Wayne Luk"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"d398dd328e4fda1fa611726f754f3ce5","permalink":"https://martinferianc.github.io/publication/fan-2018-real/","publishdate":"2022-04-03T18:26:48.040406Z","relpermalink":"/publication/fan-2018-real/","section":"publication","summary":"Convolutional neural network (CNN)-based object detection has been widely employed in various applications such as autonomous driving and intelligent video surveillance. However, the computational complexity of conventional convolution hinders its application in embedded systems. Recently, a mobile-friendly CNN model SSDLite-MobileNetV2 (SSDLiteM2) has been proposed for object detection. This model consists of a novel layer called bottleneck residual block (BRB). Although SSDLiteM2 contains far fewer parameters and computations than conventional CNN models, its performance on embedded devices still cannot meet the requirements of real-time processing. This paper proposes a novel FPGA-based architecture for SSDLiteM2 in combination with hardware optimizations including fused BRB, processing element (PE) sharing and load-balanced channel pruning. Moreover, a novel quantization scheme called partial quantization has been developed, which partially quantizes SSDLiteM2 to 8 bits with only 1.8% accuracy loss. Experiments show that the proposed design on a Xilinx ZC706 device can achieve up to 65 frames per second with 20.3 mean average precision on the COCO dataset.","tags":[""],"title":"A Real-Time Object Detection Accelerator with Compressed SSDLite on FPGA","type":"publication"}]