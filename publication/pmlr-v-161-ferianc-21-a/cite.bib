@inproceedings{pmlr-v161-ferianc21a,
 abstract = {Bayesian neural networks (BNNs) are making significant progress in many research areas where decision-making needs to be accompanied by uncertainty estimation. Being able to quantify uncertainty while making decisions is essential for understanding when the model is over-/under-confident, and hence BNNs are attracting interest in safety-critical applications, such as autonomous driving, healthcare, and robotics. Nevertheless, BNNs have not been as widely used in industrial practice, mainly because of their increased memory and compute costs. In this work, we investigate quantisation of BNNs by compressing 32-bit floating-point weights and activations to their integer counterparts, that has already been successful in reducing the compute demand in standard pointwise neural networks. We study three types of quantised BNNs, we evaluate them under a wide range of different settings, and we empirically demonstrate that a uniform quantisation scheme applied to BNNs does not substantially decrease their quality of uncertainty estimation.},
 author = {Ferianc, Martin and Maji, Partha and Mattina, Matthew and Rodrigues, Miguel},
 booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
 editor = {de Campos, Cassio and Maathuis, Marloes H.},
 month = {27--30 Jul},
 pages = {929--938},
 pdf = {https://proceedings.mlr.press/v161/ferianc21a/ferianc21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On the effects of quantisation on model uncertainty in Bayesian neural networks},
 url = {https://proceedings.mlr.press/v161/ferianc21a.html},
 volume = {161},
 year = {2021}
}

