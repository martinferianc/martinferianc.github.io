@misc{ferianc2021causal,
 abstract = {Neural networks (NNs) are making a large impact both on research and industry. Nevertheless, as NNs' accuracy increases, it is followed by an expansion in their size, required number of compute operations and energy consumption. Increase in resource consumption results in NNs' reduced adoption rate and real-world deployment impracticality. Therefore, NNs need to be compressed to make them available to a wider audience and at the same time decrease their runtime costs. In this work, we approach this challenge from a causal inference perspective, and we propose a scoring mechanism to facilitate structured pruning of NNs. The approach is based on measuring mutual information under a maximum entropy perturbation, sequentially propagated through the NN. We demonstrate the method's performance on two datasets and various NNs' sizes, and we show that our approach achieves competitive performance under challenging conditions.},
 archiveprefix = {arXiv},
 author = {Martin Ferianc and Anush Sankaran and Olivier Mastropietro and Ehsan Saboori and Quentin Cappart},
 booktitle = {The AAAI-22 Workshop on Information-Theoretic Methods for Causal Inference and Discovery},
 eprint = {2112.10229},
 primaryclass = {cs.LG},
 title = {On Causal Inference for Data-free Structured Pruning},
 url = {https://arxiv.org/abs/2112.10229},
 year = {2021}
}

